---
title: Regression with Orthonormal Polynomials
author: Michael Bigelow
date: '2019-11-30'
categories:
  - Linear Models
tags:
  - python
  - r
  - regression
  - mathematics
  - statistics
  - computing
slug: regression-with-forsythe-hayes-orthonormal-polynomials
summary: An implementation of the Forsythe-Hayes algorithm in Python, and an example
  approximating $f(x)=\sin(2\pi x),x\in[0,1]$.
authors:
  - admin
lastmod: '2019-11-30T15:35:13-07:00'
featured: yes
image:
  caption: ''
  focal_point: ''
  preview_only: no
---



<div id="overview" class="section level1">
<h1>Overview</h1>
<p>We implement the Forsythe-Hayes algorithm in Python, and use it to approximate <span class="math inline">\(f(x)=\sin(2\pi x),\;\;x\in[0,1]\)</span>. We compare the result to that found with usual polynomial regression.</p>
</div>
<div id="setup" class="section level1">
<h1>Setup</h1>
<p>To find orthonormal polynomials for our regression, we first implement the Forsythe-Hayes algorithm,
<span class="math display">\[\psi_0(x):=1\]</span>
<span class="math display">\[\psi_1(x)=x-\bar{x}\]</span>
<span class="math display">\[\psi_{r+1}(x):=(x-a_{r+1})\psi_r(x)-b_r\psi_{r-1}(x)\]</span>
in Python, taking care to normalize the orthogonal columns <span class="math inline">\(\psi_i\)</span> of the resulting design matrix to yield orthonormal <span class="math inline">\(\phi_i\)</span>:</p>
<pre class="python"><code>import numpy as np

def forsythehayes(x, poly=4):
    &#39;&#39;&#39;
    Forsythe-Hayes orthonormal polynomials
    :param x: a vector of data (numpy array)
    :param poly: number of polynomials to be used
    :return: n x d design matrix of polynomials from the 2-step method, 
              vector of a, vector of b
    &#39;&#39;&#39;
    n = len(x)
    psi = [np.ones(n) for p in range(poly + 1)]
    a = np.ones(poly + 1)
    b = np.ones(poly)
    a[1] = np.mean(x)
    psi[1] = (x - a[1])
    for r in range(1, poly):
        a[r+1] = (np.dot(x, np.square(psi[r]))) / np.sum(np.square(psi[r]))
        b[r] = np.sum(np.square(psi[r])) / np.sum(np.square(psi[r-1]))
        psi[r+1] = (x - a[r+1]) * psi[r] - b[r] * psi[r-1]
    design = np.stack(psi, axis=1)
    for i in range(poly + 1):
        design[:,i] = design[:,i] / np.linalg.norm(design[:,i])
    return design, a, b</code></pre>
<p>Again using Python, we declare our data of interest <span class="math inline">\(x_i := \frac{i}{100}\)</span> for <span class="math inline">\(i = 1, ...,100\)</span>:</p>
<pre class="python"><code>x_i = np.array([(i+1)/100 for i in range(100)])</code></pre>
<p>We then call our Forsythe-Hayes function with our <span class="math inline">\(x_i\)</span> as input, and view an R dataframe of the design matrix <span class="math inline">\(\mathbf{X}\)</span> returned by the Python function:</p>
<pre><code>## # A tibble: 100 x 5
##      `0`    `1`   `2`     `3`     `4`
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1   0.1 -0.171 0.217 -0.249   0.271 
##  2   0.1 -0.168 0.204 -0.219   0.217 
##  3   0.1 -0.165 0.191 -0.190   0.167 
##  4   0.1 -0.161 0.178 -0.163   0.122 
##  5   0.1 -0.158 0.166 -0.137   0.0813
##  6   0.1 -0.154 0.154 -0.113   0.0452
##  7   0.1 -0.151 0.142 -0.0904  0.0132
##  8   0.1 -0.147 0.131 -0.0690 -0.0150
##  9   0.1 -0.144 0.119 -0.0489 -0.0395
## 10   0.1 -0.140 0.108 -0.0302 -0.0605
## # … with 90 more rows</code></pre>
<p>If we are curious about the <span class="math inline">\(a_{r+1}\)</span> and <span class="math inline">\(b_r\)</span> values that our function used at each iteration to compute these polynomials, we can easily view them:</p>
<pre><code>## a =  [0.505 0.505 0.505 0.505]</code></pre>
<pre><code>## b =  [1.     0.0833 0.0666 0.0642]</code></pre>
<p>To show that our columns <span class="math inline">\(\phi_i\)</span> are orthonormal,
<span class="math display">\[\sum_{j=1}^{100}\phi_k(x_j)\phi_l(x_j)= \begin{cases} 0 &amp; k \neq l \\
      1 &amp; k = l 
   \end{cases}\]</span>
it suffices to show that for the design matrix <span class="math inline">\(\mathbf{X}\)</span> computed by our algorithm, <span class="math inline">\(\mathbf{X&#39;X}=\mathbf{I}\)</span>:</p>
<pre class="r"><code>round(t(as.matrix(forsythedesign)) %*% as.matrix(forsythedesign), 6)</code></pre>
<pre><code>##   0 1 2 3 4
## 0 1 0 0 0 0
## 1 0 1 0 0 0
## 2 0 0 1 0 0
## 3 0 0 0 1 0
## 4 0 0 0 0 1</code></pre>
</div>
<div id="approximation" class="section level1">
<h1>Approximation</h1>
<p>We wish to approximate
<span class="math display">\[f(x)=\sin(2\pi x),\;\;x\in[0,1]\]</span>
with a polynomial of degree 4 using the design matrix we computed above. Since <span class="math inline">\(\mathbf{X&#39;X}=\mathbf{I}\)</span>, our parameter estimates <span class="math inline">\(\hat{\boldsymbol{\gamma}}\)</span> are simplified from the general case of linear models: <span class="math display">\[\hat{\boldsymbol{\gamma}}=(\mathbf{X&#39;X})^{-1}\mathbf{X&#39;Y}\]</span> <span class="math display">\[=\mathbf{X&#39;Y}.\]</span>
We can quickly compute our “observed” <span class="math inline">\(\mathbf{Y}\)</span> values for each of our <span class="math inline">\(x_i\)</span> using Python’s list comprehensions:</p>
<pre class="python"><code>Y = np.array([np.sin(2 * np.pi * x_i[i]) for i in range(len(x_i))])</code></pre>
<p>Then fit the model in R:</p>
<pre class="r"><code>Y &lt;- py$Y
gamma &lt;- t(as.matrix(forsythedesign)) %*% Y
gamma</code></pre>
<pre><code>##            [,1]
## 0 -8.867770e-17
## 1 -5.511751e+00
## 2  2.135119e-01
## 3  4.374314e+00
## 4 -6.092185e-02</code></pre>
</div>
<div id="comparison-to-usual-regression" class="section level1">
<h1>Comparison to usual regression</h1>
<p>If instead we had used polynomials <span class="math inline">\(1, x, x^2, x^3, x^4\)</span> for our design matrix, and fitted our model in the usual way, i.e. <span class="math display">\[\hat{\boldsymbol{\beta}}=(\mathbf{X&#39;X})^{-1}\mathbf{X&#39;Y}\]</span>
we would have design matrix (from Python):</p>
<pre class="python"><code>design = pd.DataFrame(np.stack([np.power(x_i, i) for i in range(5)], axis=1))</code></pre>
<p>viewed (in R):</p>
<pre class="r"><code>design &lt;- py$design
as.tibble(design)</code></pre>
<pre><code>## # A tibble: 100 x 5
##      `0`   `1`    `2`       `3`         `4`
##    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
##  1     1  0.01 0.0001 0.000001  0.00000001 
##  2     1  0.02 0.0004 0.000008  0.00000016 
##  3     1  0.03 0.0009 0.0000270 0.000000810
##  4     1  0.04 0.0016 0.000064  0.00000256 
##  5     1  0.05 0.0025 0.000125  0.00000625 
##  6     1  0.06 0.0036 0.000216  0.0000130  
##  7     1  0.07 0.0049 0.000343  0.0000240  
##  8     1  0.08 0.0064 0.000512  0.0000410  
##  9     1  0.09 0.0081 0.000729  0.0000656  
## 10     1  0.1  0.01   0.001     0.0001     
## # … with 90 more rows</code></pre>
<p>and our parameter vector <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>:</p>
<pre class="r"><code>Design &lt;- as.matrix(design)
beta &lt;- solve(t(Design) %*% Design) %*% t(Design) %*% Y
beta</code></pre>
<pre><code>##          [,1]
## 0  -0.2356596
## 1  12.4317884
## 2 -36.4913890
## 3  25.7510953
## 4  -1.2812805</code></pre>
</div>
<div id="comparing-the-models" class="section level1">
<h1>Comparing the models</h1>
<p>We can estimate <span class="math inline">\(\hat{\mathbf{Y}}_{F}\)</span> from our Forsythe-Hayes orthonormal polynomial design matrix <span class="math inline">\(\mathbf{X}_{F}\)</span> and <span class="math inline">\(\hat{\boldsymbol{\gamma}}\)</span>:
<span class="math display">\[\hat{\mathbf{Y}}_{F}=\mathbf{X}_{F} \hat{\boldsymbol{\gamma}}\]</span>
and <span class="math inline">\(\hat{\mathbf{Y}}\)</span> from our usual polynomial design matrix <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>:
<span class="math display">\[\hat{\mathbf{Y}}=\mathbf{X} \hat{\boldsymbol{\beta}}\]</span>
and preview the data and the models’ predictions:</p>
<pre class="r"><code>x &lt;- py$x_i
Forsythe_Hayes &lt;- as.matrix(forsythedesign) %*% gamma
Poly_Regression &lt;- Design %*% beta
comparison &lt;- tibble(x, Y, Forsythe_Hayes, Poly_Regression)
comparison</code></pre>
<pre><code>## # A tibble: 100 x 4
##        x      Y Forsythe_Hayes[,1] Poly_Regression[,1]
##    &lt;dbl&gt;  &lt;dbl&gt;              &lt;dbl&gt;               &lt;dbl&gt;
##  1  0.01 0.0628           -0.115              -0.115  
##  2  0.02 0.125            -0.00141            -0.00141
##  3  0.03 0.187             0.105               0.105  
##  4  0.04 0.249             0.205               0.205  
##  5  0.05 0.309             0.298               0.298  
##  6  0.06 0.368             0.384               0.384  
##  7  0.07 0.426             0.465               0.465  
##  8  0.08 0.482             0.538               0.538  
##  9  0.09 0.536             0.606               0.606  
## 10  0.1  0.588             0.668               0.668  
## # … with 90 more rows</code></pre>
<p>Upon inspection, the two approximations appear to yield identical results.</p>
<p>We now plot <span class="math inline">\(f(x)=\sin(2\pi x),\;\;x\in[0,1]\)</span> in black and our two approximations in orange and blue:</p>
<pre class="r"><code>comparison %&gt;%
  ggplot(aes(x)) + 
    geom_point(aes(x, Y)) +
    geom_line(aes(y = Poly_Regression), color = &#39;blue&#39;, size = 1.5) +
    geom_line(aes(y = Forsythe_Hayes), color = &#39;orange&#39;) </code></pre>
<p><img src="/post/2020-02-01-regression-rmd_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Our models both fit the function equally well over the specified interval; however if we wish to approximate the curve with a polynomial of higher degree, the Forsythe-Hayes model will avoid returning a design matrix that is ill-conditioned.</p>
<p>For curiosity’s sake, additional tests were carried out. On this data, the Forsythe-Hayes algorithm successfully calculated design matrices for regression with polynomials of degree 100 without issue, whereas the usual polynomial regression failed past degree ten due to an ill-conditioned design matrix. While such high-degree polynomials are certainly not needed for this data (merely increasing the degree to five fits the data quite well), such an exercise illustrates the comparative robustness of the Forsythe-Hayes approach.</p>
</div>
