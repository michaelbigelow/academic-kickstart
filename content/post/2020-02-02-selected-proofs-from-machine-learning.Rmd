---
title: Selected Proofs from Machine Learning
author: Michael Bigelow
date: '2019-11-06'
slug: selected-proofs-from-machine-learning
categories:
  - Machine Learning
tags:
  - mathematics
  - machine learning
  - proofs
  - VC dimension
subtitle: ''
summary: 'Four key proofs from a graduate course in machine learning.'
authors: [admin]
lastmod: '2019-11-06T00:54:26-07:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

# Background
The following are several proof solutions I wrote during a graduate machine learning course during a unit covering statistical/computational learning theory, specifically concerning Vapnik-Chervonenkis dimension of a hypothesis class of learners.

# Proof 1: Simple Classifier
Consider a hypothesis class $\mathcal{H}$ defined as a union of two real valued intervals [a, b] and [c, d] on the number line. A point $x$ is labeled as positive if it is inside in either of the two intervals, otherwise it is labeled as negative; i.e. $x$ is "+" if and only if $a \le x \le b$ or  $c \le x \le d$. Determine the VC dimension of the hypothesis class $\mathcal{H}$.

## Solution
To show that $VC(\mathcal{H}) \geqslant 4$, we consider four distinct points on the real line: $x_1 < x_2 < x_3 < x_4$.  Clearly we can label zero points positive by setting 
$$a=b=c=d\neq x_i \forall i.$$  
We can label all points positive by setting 
$$a=c=x_1,b=d=x_4.$$ 
We can label any singleton point $x_j$ positive by setting 
$$a=b=x_j, c=d\neq x_i \forall i\neq j.$$
We can label any two $adjacent$ points $x_j<x_k$ positive with 
$$c=x_j < x_k = d.$$
And to label any two points $x_j, x_k$ positive in the case that $\exists x_i: x_j<x_i<x_k,$ we define two intervals:
$$a=x_j\leqslant b < x_i < c \leqslant x_k = d.$$
We can label any three points positive by observing that in our set of four points, two of the three positive points must be adjacent, so we construct our interval $[a,b]$ according to the rules above for a singleton point and $[c,d]$ for a pair of adjacent points. 

To show that $VC(\mathcal{H}) < 5$, notice that if we have $x_j=x_k$ for some $j,k \in \mathbb{Z}: \;1 \leqslant j<k\leqslant 5,$ then we fail to shatter $S$ because we may have $f(x_j)\neq f(x_k)$ and we know that $$\forall  x_j=x_k,  h(x_j)=h(x_k).$$  So WLOG we consider a set of distinct $x_i: x_1 < x_2 < x_3 < x_4 < x_5.$  
Now consider three nonadjacent positively-labeled points $f(x_1)=f(x_3)=f(x_5)=1.$  Recall that to label a set of three points positive using our $h \in \mathcal{H},$ we made use of the fact that two of the points had to be adjacent, which is no longer true.  In particular, to account for 
$$f(x_1)=f(x_5)=1,f(x_2)=f(x_4)=0$$
we must have
$$a \leqslant x_1 \leqslant b < x_2 <x_3 <x_4 <c \leqslant x_5 \leqslant d$$ and in this case there is no $h \in \mathcal{H}$ such that $h(x_3)=1$. Hence $VC(\mathcal{H}) = 4$. $\square$

# Proof 2: Simple Classifier
Consider a more general hypothesis class $\mathcal{H}$ consisting of a union of $k$ real-valued intervals. Determine the VC dimension of the hypothesis class $\mathcal{H}$.

## Solution
The above argument shows that for five points we needed another interval to shatter $S,$ namely three intervals.  Indeed, it is easy to see that three intervals will suffice for $d=6$ points as well, since any set of four or more positively-labeled points will contain two or more adjacent positively-labeled points which can be covered by a single interval, leaving two intervals to cover the remaining points (of which there will be at most two).

In general, for $d$ points, where $d$ is an even integer, the union of $k=\frac{d}{2}$ intervals can generate any labeling, since whenever the number of positive points exceeds $k=\frac{d}{2}$, we are guaranteed that at least two of them will be adjacent and can be therefore labeled positive by a single interval.  So for $\mathcal{H}$ consisting of all unions of k real-valued intervals, 
$$VC(\mathcal{H})\geqslant d = 2k.$$

Again, by the reasoning above WLOG we can restrict our attention to a set of distinct $x_i: 1\leqslant i \leqslant d.$ To show that 
$$VC(\mathcal{H})< d = 2k+1$$
notice that d is an odd integer and for the labeling that makes alternating points positive such that $f(x_1)=f(x_3)=...=f(x_d)=1$ with all other other points negative, there will be $\frac{d+1}{2}$ positive, non-adjacent points, each requiring its own interval; in particular, we will need 
$$\frac{(2k+1)+1}{2} = \frac{2k+2}{2} = k+1$$ intervals, and we only have k, so $\mathcal{H}$ fails to shatter the set $S$ of size $d.$

Hence for a hypothesis class $\mathcal{H}$ consisting of a union of $k$ real-valued intervals, $VC(\mathcal{H})= 2k.$ $\square$

# Proof 3: Comparing Hypothesis Classes
Given two hypothesis classes, $\mathcal{H}$ and $\mathcal{G}$, prove that if $\mathcal{H} \subseteq \mathcal{G}$ then $VC(\mathcal{H})\leq VC(\mathcal{G})$.

## Solution
Let $\mathcal{H}\subseteq \mathcal{G}$ and assume $VC(\mathcal{H})>VC(\mathcal{G})$ toward a contradiction.  
This implies that for any arrangement of $d=VC(\mathcal{H})$ points $x \in S,$ there is no $g\in \mathcal{G}$ such that $g(x)=f(x).$  But we know that for some arrangement of $d=VC(\mathcal{H})$ points $x \in S$ there is a function $h \in \mathcal{H}$ such that $h(x)=f(x)$ by definition of $VC(\mathcal{H}).$  But this is a contradition, since $\mathcal{H}\subseteq \mathcal{G}$ and the definition of subset means that 
$\forall \;h \in \mathcal{H}, \exists \;g \in \mathcal{G}$ such that $h(x)=g(x) \;\forall\;x \in S. \square$

# Proof 4: VC Dimension and Hypothesis Class Size
Let $\mathcal{H}$ be a finite concept class, show that if $VC(\mathcal{H})=d$, then $|\mathcal{H}|\geq 2^d$.

## Solution
Let $\mathcal{H}$ be a finite concept class such that $VC(\mathcal{H})=d$. For a direct proof that $|\mathcal{H}|\geq 2^d$, first note that every possible labeling of $x \in S$ given by $f$ is equivalent to a partition of $S$ into a set of positive and a set of negative points, which we'll denote $S^+$ and $S^-:$
$$S^+:=\{x \in S:f(x)=1\}$$ 
$$S^-:=\{x \in S:f(x)=0\}$$
Recall from set theory that for a set $S$ the "power set of $S$" is defined as the class of all possible subsets of $S,$ denoted $\mathcal{P}(S)$. By the definition of the power set of $S,$ it is clear that
$$S^+ \in \mathcal{P}(S)\; \forall S^+$$
Notice that $VC(\mathcal{H})=d$ implies that for $d$ points, the "restriction of $\mathcal{H}$ to $S$" (as defined in Shalev-Shwarz and Ben-David, 2014. ["Understanding Machine Learning"](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf), p.69), denoted $\mathcal{H}_S,$ is identically the power set of $S.$ Hence we have 
$$|\mathcal{H}_S|=|\mathcal{P}(S)|.$$ Since $\mathcal{H}_S \subseteq \mathcal{H},$ we have
$$|\mathcal{H}|\geq |\mathcal{P}(S)|$$
and we know that the size of the power set $\mathcal{P}(S)$ of any set $S$ is $2^{|S|}=2^d$ which implies that  
$$|\mathcal{H}|\geq 2^d.\square$$
