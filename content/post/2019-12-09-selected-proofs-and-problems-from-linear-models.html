---
title: Selected Proofs from Linear Models
author: Michael Bigelow
date: '2019-12-09'
slug: selected-proofs-from-linear-models
categories:
  - Linear Models
tags:
  - regression
  - proofs
  - mathematics
  - linear algebra
  - statistics
subtitle: ''
summary: 'Five key proofs from a graduate course in linear models.'
authors: [admin]
lastmod: '2019-12-09T17:56:24-07:00'
featured: yes
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Below follow several proofs I wrote throughout a graduate course in linear models. The proofs range from linear algebra concepts to construction of linear models given data with certain properties.</p>
</div>
<div id="proof-1-eigenvalues-of-projection-matrices" class="section level1">
<h1>Proof 1: Eigenvalues of Projection Matrices</h1>
<p>Let <span class="math inline">\(P\)</span> be a projection matrix and let <span class="math inline">\(\lambda\)</span> be an eigenvalue of <span class="math inline">\(P\)</span>. Prove that <span class="math inline">\(\lambda \in \{0,1\}\)</span></p>
<div id="solution" class="section level2">
<h2>Solution</h2>
<p>Recall that a projection matrix is <span class="math inline">\(symmetric\)</span> and <span class="math inline">\(idempotent\)</span>; that is, for a projection matrix <span class="math inline">\(P\)</span>, <span class="math inline">\(P&#39;=P\)</span> and <span class="math inline">\(P^2=P\)</span>. For the eigenvector <span class="math inline">\(\mathbf{v}\)</span>, we have
<span class="math display">\[P\mathbf{v} = \lambda \mathbf{v}\]</span>
by the definition of eigenvalue, which implies that
<span class="math display">\[P^2 \mathbf{v} = \lambda P \mathbf{v} = \lambda^2\mathbf{v}\]</span>
but because <span class="math inline">\(P\)</span> is idempotent,
<span class="math display">\[P^2\mathbf{v} = P\mathbf{v} = \lambda \mathbf{v}\]</span>
and since eigenvectors are by definition nonzero, we get
<span class="math display">\[\lambda^2=\lambda\]</span>
which only makes sense if <span class="math inline">\(\lambda \in \{0,1\}\)</span>.</p>
</div>
</div>
<div id="proof-2-multivariate-standard-normal" class="section level1">
<h1>Proof 2: Multivariate Standard Normal</h1>
<p>Let <span class="math inline">\(X \sim N_3(0,I_3)\)</span>. Prove that
<span class="math display">\[\frac{1}{3}[(X_1-X_2)^2 + (X_2-X_3)^2 + (X_1-X_3)^2] \sim \chi^2_2.\]</span></p>
<div id="solution-1" class="section level2">
<h2>Solution</h2>
<p>Expanding the above expression yields
<span class="math display">\[\frac{1}{3}[2X_1^2 +2X_2^2 +2X_3^2 - 2X_1X_2 -2X_2X_3-2X_1X_3].\]</span>
Let
<span class="math display">\[A= \frac{1}{3} \begin{bmatrix} 
2 &amp; -1 &amp; -1 \\
-1 &amp; 2 &amp; -1 \\
-1 &amp; -1 &amp; 2
\end{bmatrix}.\]</span>
Then <span class="math inline">\(A\)</span> is the matrix associated to the above quadratic form, which we denote <span class="math inline">\(X&#39;AX\)</span>. We can perform the spectral decomposition
<span class="math display">\[X&#39;AX = X&#39;PDP&#39;X.\]</span>
Let <span class="math inline">\(Z=P&#39;X\)</span>. Then we have
<span class="math display">\[=DZ^2.\]</span>
But <span class="math inline">\(A\)</span> is idempotent, so we have <span class="math inline">\(A^2 = A\)</span>, which means that all its eigenvalues are either zero or one, and since <span class="math inline">\(X_i \sim N(0,1)\; \forall i\)</span> we know <span class="math inline">\(Z_i^2 \sim \chi^2\)</span>, which means the above is a sum of independent chi-squared random variables:
<span class="math display">\[=\sum_{i=1}^{3} d_iZ_i^2.\]</span>
Since <span class="math inline">\(A\)</span> is a projection matrix, <span class="math inline">\(\text{rank}(A) = \text{tr}(A) = 2\)</span> and so it has two eigenvalues <span class="math inline">\(\lambda = 1\)</span> and one eigenvalue <span class="math inline">\(\lambda=0\)</span>. Hence in the expression above, <span class="math inline">\(d_1 = d_2 = 1\)</span> and <span class="math inline">\(d_3=0\)</span>, meaning we have a sum of two independent chi-squared random variables, which is distributed <span class="math inline">\(\chi^2_2\)</span>.</p>
</div>
</div>
<div id="proof-3-standard-multivariate-normal" class="section level1">
<h1>Proof 3: Standard Multivariate Normal</h1>
<p>Let <span class="math inline">\(X \sim N_3(0,I_3)\)</span> and let
<span class="math display">\[Y_1=X_1+X_2,\]</span>
<span class="math display">\[Y_2=X_1-X_2+X_3,\]</span>
<span class="math display">\[Y_3=X_1-X_2-2X_3.\]</span>
Prove that <span class="math inline">\(Y_1\)</span>, <span class="math inline">\(Y_2\)</span>, and <span class="math inline">\(Y_3\)</span> are independent. What are their distributions?</p>
<div id="solution-2" class="section level2">
<h2>Solution</h2>
<p>Let
<span class="math display">\[A=\begin{bmatrix} 
1 &amp; 1 &amp; 0 \\
1 &amp; -1 &amp; 1 \\
1 &amp; -1 &amp; -2
\end{bmatrix}.\]</span>
Then <span class="math inline">\(Y=AX\)</span> and <span class="math inline">\(\mathbf{Var}[Y] = \mathbf{Var}[AX] = A\mathbf{Var}[X]A&#39;\)</span>.</p>
<p>Since <span class="math inline">\(X_i\)</span> are iid, <span class="math inline">\(\mathbf{Cov}(X_i, X_j)=0\;\forall i\neq j\)</span> and since <span class="math inline">\(X_i \sim N(0,1)\)</span>, <span class="math inline">\(\mathbf{Cov}(X_i, X_i)=1\;\forall i\)</span>.</p>
<p>Hence,
<span class="math display">\[\mathbf{Var}[Y]=\begin{bmatrix} 
1 &amp; 1 &amp; 0 \\
1 &amp; -1 &amp; 1 \\
1 &amp; -1 &amp; -2
\end{bmatrix}\begin{bmatrix} 
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 1
\end{bmatrix}\begin{bmatrix} 
1 &amp; 1 &amp; 0 \\
1 &amp; -1 &amp; -1 \\
1 &amp; 1 &amp; -2
\end{bmatrix}\]</span>
<span class="math display">\[=\begin{bmatrix} 
1 &amp; 1 &amp; 0 \\
1 &amp; -1 &amp; 1 \\
1 &amp; -1 &amp; -2
\end{bmatrix}\begin{bmatrix} 
1 &amp; 1 &amp; 1 \\
1 &amp; -1 &amp; -1 \\
0 &amp; 1 &amp; -2
\end{bmatrix}\]</span>
<span class="math display">\[=\begin{bmatrix} 
2 &amp; 0 &amp; 0 \\
0 &amp; 3 &amp; 0 \\
0 &amp; 0 &amp; 6
\end{bmatrix} = \Sigma,\]</span>
which shows that <span class="math inline">\(\mathbf{Cov}[Y_i, Y_j]=0\; \forall i \neq j\)</span>. Hence <span class="math inline">\(Y_i\)</span> are independent because <span class="math inline">\(Y \sim N(0, \Sigma)\)</span>. In particular,
<span class="math display">\[Y_1 \sim N(0,2),\]</span>
<span class="math display">\[Y_2 \sim N(0,3),\]</span>
<span class="math display">\[Y_3 \sim N(0,6).\]</span></p>
</div>
</div>
<div id="proof-4-general-linear-model" class="section level1">
<h1>Proof 4: General Linear Model</h1>
<p>Suppose <span class="math inline">\(Y_{1,1},...,Y_{1,n}\)</span> is an iid sample from a <span class="math inline">\(N(\beta,\sigma^2)\)</span> population, and <span class="math inline">\(Y_{2,1},...,Y_{2,n}\)</span> an independent iid sample from a <span class="math inline">\(N(3\beta,\sigma^2)\)</span> population.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Is this a general linear model? Explain carefully.</p></li>
<li><p>If your answer to (a) was “yes,” then find the least-squares estimator of <span class="math inline">\(\beta\)</span>, and write a t-test for <span class="math inline">\(H_0: \beta = 0\)</span>. Your test must be described explicitly in terms of the <span class="math inline">\(Y_{i,j}\)</span>’s.</p></li>
</ol>
<div id="solution-3" class="section level2">
<h2>Solution</h2>
<ol style="list-style-type: lower-alpha">
<li><p>Yes, it is a general linear model <span class="math inline">\(Y=X \beta + \varepsilon\)</span> with
<span class="math display">\[Y&#39;=[Y_{1,1},...,Y_{1,n},Y_{2,1},...,Y_{2,n}],\;\;\;X&#39; = [1, ...,1,3,...,3]_{1\times2n},\;\;\beta=\mu,\;\;\; \varepsilon&#39;=[\varepsilon_{1,1},...,\varepsilon_{1,n},\varepsilon_{2,1},...,\varepsilon_{2,n}]\]</span>
where <span class="math inline">\(Y\)</span>, <span class="math inline">\(X\)</span>, and <span class="math inline">\(\varepsilon\)</span> are <span class="math inline">\(2n \times 1\)</span> and <span class="math inline">\(\beta\)</span> is <span class="math inline">\(1 \times 1\)</span>, so <span class="math inline">\(p=1\)</span>, and <span class="math inline">\(\varepsilon_{i,j} \sim N(0,\sigma^2)\)</span>.</p></li>
<li><p>Then the least-squares estimate of <span class="math inline">\(\beta\)</span> is
<span class="math display">\[\hat{\beta} = (X&#39;X)^{-1}X&#39;Y.\]</span>
We have
<span class="math display">\[X&#39;X = 1^2n + 3^2n = 10n\]</span>
<span class="math display">\[X&#39;Y = \sum_{j=1}^{n}Y_{1,j} + 3\sum_{j=1}^{n}Y_{2,j}\]</span>
so we have
<span class="math display">\[\hat{\beta} = \frac{1}{10n} \left(\sum_{j=1}^{n}Y_{1,j} + 3\sum_{j=1}^{n}Y_{2,j}\right).\]</span>
We know a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\beta\)</span> is given by
<span class="math display">\[\hat{\beta} \pm St^{\alpha/2}_{2n-1} \sqrt{X&#39;X}\]</span>
where <span class="math inline">\(S^2:= \frac{1}{2n-1} RSS\)</span> is computed
<span class="math display">\[S^2= \frac{1}{2n-1} \left[\sum_{j=1}^{n}(Y_{1,j}-\hat{\beta})^2 + \sum_{j=1}^{n}(Y_{2,j}-3\hat{\beta})^2\right].\]</span></p></li>
</ol>
</div>
</div>
<div id="proof-5-general-linear-model" class="section level1">
<h1>Proof 5: General Linear Model</h1>
<p>Suppose <span class="math inline">\(Y_{1,1},...,Y_{1,n}\)</span> is an iid sample from a <span class="math inline">\(N(\beta,\sigma^2)\)</span> population, and <span class="math inline">\(Y_{2,1},...,Y_{2,n}\)</span> an independent iid sample from a <span class="math inline">\(N(\beta,3\sigma^2)\)</span> population.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Is this a general linear model? Explain carefully.</p></li>
<li><p>If your answer to (a) was “yes,” then find the least-squares estimator of <span class="math inline">\(\beta\)</span>, and write a t-test for <span class="math inline">\(H_0: \beta = 0\)</span>. Your test must be described explicitly in terms of the <span class="math inline">\(Y_{i,j}\)</span>’s.</p></li>
</ol>
<div id="solution-4" class="section level2">
<h2>Solution</h2>
<ol style="list-style-type: lower-alpha">
<li><p>Yes, it is a general linear model <span class="math inline">\(Y=X \beta + \varepsilon\)</span> if we make one small adjustment to achieve <span class="math inline">\(\varepsilon_{i,j} \sim N(0,\sigma^2)\)</span>, since as originally stated, <span class="math inline">\(\varepsilon_{2,j} \sim N(0,3\sigma^2)\)</span>. Let the original <span class="math inline">\(Y_{2,j} \;(\sim N(\beta,3\sigma^2))\)</span> be denoted <span class="math inline">\(\widetilde{Y}_{2,j}\;(\sim N(\beta,3\sigma^2))\)</span> and define our new <span class="math inline">\(Y_{2,j} = \frac{\widetilde{Y}_{2,j}}{\sqrt3}\;(\sim N(\beta,\sigma^2))\)</span>
so we have
<span class="math display">\[Y&#39;=[Y_{1,1},...,Y_{1,n},Y_{2,1},...,Y_{2,n}],\;\;\;X&#39; = \left[1, ...,1,\frac{1}{\sqrt3},...,\frac{1}{\sqrt3}\right]_{1\times2n},\;\;\beta=\mu,\;\;\; \varepsilon&#39;=[\varepsilon_{1,1},...,\varepsilon_{1,n},\varepsilon_{2,1},...,\varepsilon_{2,n}]\]</span>
where <span class="math inline">\(Y\)</span>, <span class="math inline">\(X\)</span>, and <span class="math inline">\(\varepsilon\)</span> are <span class="math inline">\(2n \times 1\)</span> and <span class="math inline">\(\beta\)</span> is <span class="math inline">\(1 \times 1\)</span>, so <span class="math inline">\(p=1\)</span>.</p></li>
<li><p>Then the least-squares estimate of <span class="math inline">\(\beta\)</span> is
<span class="math display">\[\hat{\beta} = (X&#39;X)^{-1}X&#39;Y.\]</span>
We have
<span class="math display">\[X&#39;X = n + \frac{1}{3}n\]</span>
<span class="math display">\[X&#39;Y = \sum_{j=1}^{n}Y_{1,j} + \frac{1}{\sqrt3}\sum_{j=1}^{n}Y_{2,j}\]</span>
so we have
<span class="math display">\[\hat{\beta} = \frac{3}{4n} \left(\sum_{j=1}^{n}Y_{1,j} + \frac{1}{\sqrt3}\sum_{j=1}^{n}Y_{2,j}\right).\]</span>
We know a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\beta\)</span> is given by
<span class="math display">\[\hat{\beta} \pm St^{\alpha/2}_{2n-1} \sqrt{X&#39;X}\]</span>
and <span class="math inline">\(S^2:= \frac{1}{2n-1} RSS\)</span> is computed
<span class="math display">\[S^2= \frac{1}{2n-1} \left[\sum_{j=1}^{n}(Y_{1,j}-\hat{\beta})^2 + \sum_{j=1}^{n}\left(Y_{2,j}-\frac{\hat{\beta}}{\sqrt3}\right)^2\right].\]</span></p></li>
</ol>
</div>
</div>
