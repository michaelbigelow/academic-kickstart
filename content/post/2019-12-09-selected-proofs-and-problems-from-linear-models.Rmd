---
title: Selected Proofs from Linear Models
author: Michael Bigelow
date: '2019-12-09'
slug: selected-proofs-from-linear-models
categories:
  - Linear Models
tags:
  - regression
  - proofs
  - mathematics
  - linear algebra
subtitle: ''
summary: 'Five key proofs from a graduate course in linear models.'
authors: [admin]
lastmod: '2019-12-09T17:56:24-07:00'
featured: yes
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
Below follow several proofs I wrote throughout a graduate course in linear models.  The proofs range from linear algebra concepts to construction of linear models given data with certain properties.

# Proof 1: Eigenvalues of Projection Matrices
Let $P$ be a projection matrix and let $\lambda$ be an eigenvalue of $P$.  Prove that $\lambda \in \{0,1\}$

## Solution
Recall that a projection matrix is $symmetric$ and $idempotent$; that is, for a projection matrix $P$, $P'=P$ and $P^2=P$.  For the eigenvector $\mathbf{v}$, we have
$$P\mathbf{v} = \lambda \mathbf{v}$$
by the definition of eigenvalue, which implies that
$$P^2 \mathbf{v} = \lambda P \mathbf{v} = \lambda^2\mathbf{v}$$
but because $P$ is idempotent, 
$$P^2\mathbf{v} = P\mathbf{v} = \lambda \mathbf{v}$$
and since eigenvectors are by definition nonzero, we get
$$\lambda^2=\lambda$$
which only makes sense if $\lambda \in \{0,1\}$.

# Proof 2: Multivariate Standard Normal
Let $X \sim N_3(0,I_3)$. Prove that
$$\frac{1}{3}[(X_1-X_2)^2 + (X_2-X_3)^2 + (X_1-X_3)^2] \sim \chi^2_2.$$

## Solution
Expanding the above expression yields
$$\frac{1}{3}[2X_1^2 +2X_2^2 +2X_3^2 - 2X_1X_2 -2X_2X_3-2X_1X_3].$$
Let
$$A= \frac{1}{3} \begin{bmatrix} 
2 & -1 & -1 \\
-1 & 2 & -1 \\
-1 & -1 & 2
\end{bmatrix}.$$
Then $A$ is the matrix associated to the above quadratic form, which we denote $X'AX$. We can perform the spectral decomposition 
$$X'AX = X'PDP'X.$$
Let $Z=P'X$. Then we have
$$=DZ^2.$$
But $A$ is idempotent, so we have $A^2 = A$, which means that all its eigenvalues are either zero or one, and since $X_i \sim N(0,1)\; \forall i$ we know $Z_i^2 \sim \chi^2$, which means the above is a sum of independent chi-squared random variables:
$$=\sum_{i=1}^{3} d_iZ_i^2.$$
Since $A$ is a projection matrix, $\text{rank}(A) = \text{tr}(A) = 2$ and so it has two eigenvalues $\lambda = 1$ and one eigenvalue $\lambda=0$.  Hence in the expression above, $d_1 = d_2 = 1$ and $d_3=0$, meaning we have a sum of two independent chi-squared random variables, which is distributed $\chi^2_2$.

# Proof 3: Standard Multivariate Normal
Let $X \sim N_3(0,I_3)$ and let 
$$Y_1=X_1+X_2,$$
$$Y_2=X_1-X_2+X_3,$$
$$Y_3=X_1-X_2-2X_3.$$
Prove that $Y_1$, $Y_2$, and $Y_3$ are independent.  What are their distributions?

## Solution
Let 
$$A=\begin{bmatrix} 
1 & 1 & 0 \\
1 & -1 & 1 \\
1 & -1 & -2
\end{bmatrix}.$$
Then $Y=AX$ and $\mathbf{Var}[Y] = \mathbf{Var}[AX] = A\mathbf{Var}[X]A'$.  

Since $X_i$ are iid, $\mathbf{Cov}(X_i, X_j)=0\;\forall i\neq j$ and since $X_i \sim N(0,1)$, $\mathbf{Cov}(X_i, X_i)=1\;\forall i$.  

Hence,
$$\mathbf{Var}[Y]=\begin{bmatrix} 
1 & 1 & 0 \\
1 & -1 & 1 \\
1 & -1 & -2
\end{bmatrix}\begin{bmatrix} 
1 & 0 & 0 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{bmatrix}\begin{bmatrix} 
1 & 1 & 0 \\
1 & -1 & -1 \\
1 & 1 & -2
\end{bmatrix}$$
$$=\begin{bmatrix} 
1 & 1 & 0 \\
1 & -1 & 1 \\
1 & -1 & -2
\end{bmatrix}\begin{bmatrix} 
1 & 1 & 1 \\
1 & -1 & -1 \\
0 & 1 & -2
\end{bmatrix}$$
$$=\begin{bmatrix} 
2 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & 6
\end{bmatrix} = \Sigma,$$
which shows that $\mathbf{Cov}[Y_i, Y_j]=0\; \forall i \neq j$.  Hence $Y_i$ are independent because $Y \sim N(0, \Sigma)$.  In particular,
$$Y_1 \sim N(0,2),$$
$$Y_2 \sim N(0,3),$$
$$Y_3 \sim N(0,6).$$

# Proof 4: General Linear Model
Suppose $Y_{1,1},...,Y_{1,n}$ is an iid sample from a $N(\beta,\sigma^2)$ population, and $Y_{2,1},...,Y_{2,n}$ an independent iid sample from a $N(3\beta,\sigma^2)$ population.

(a) Is this a general linear model? Explain carefully.

(b) If your answer to (a) was "yes," then find the least-squares estimator of $\beta$, and write a t-test for $H_0: \beta = 0$.  Your test must be described explicitly in terms of the $Y_{i,j}$'s.

## Solution
(a) Yes, it is a general linear model $Y=X \beta + \varepsilon$ with
$$Y'=[Y_{1,1},...,Y_{1,n},Y_{2,1},...,Y_{2,n}],\;\;\;X' = [1, ...,1,3,...,3]_{1\times2n},\;\;\beta=\mu,\;\;\; \varepsilon'=[\varepsilon_{1,1},...,\varepsilon_{1,n},\varepsilon_{2,1},...,\varepsilon_{2,n}]$$
where $Y$, $X$, and $\varepsilon$ are $2n \times 1$ and $\beta$ is $1 \times 1$, so $p=1$, and $\varepsilon_{i,j} \sim N(0,\sigma^2)$.

(b) Then the least-squares estimate of $\beta$ is
$$\hat{\beta} = (X'X)^{-1}X'Y.$$
We have
$$X'X = 1^2n + 3^2n = 10n$$
$$X'Y = \sum_{j=1}^{n}Y_{1,j} + 3\sum_{j=1}^{n}Y_{2,j}$$
so we have 
$$\hat{\beta} = \frac{1}{10n} \left(\sum_{j=1}^{n}Y_{1,j} + 3\sum_{j=1}^{n}Y_{2,j}\right).$$
We know a $100(1-\alpha)\%$ confidence interval for $\beta$ is given by 
$$\hat{\beta} \pm St^{\alpha/2}_{2n-1} \sqrt{X'X}$$
where $S^2:= \frac{1}{2n-1} RSS$ is computed
$$S^2= \frac{1}{2n-1} \left[\sum_{j=1}^{n}(Y_{1,j}-\hat{\beta})^2 + \sum_{j=1}^{n}(Y_{2,j}-3\hat{\beta})^2\right].$$

# Proof 5: General Linear Model
Suppose $Y_{1,1},...,Y_{1,n}$ is an iid sample from a $N(\beta,\sigma^2)$ population, and $Y_{2,1},...,Y_{2,n}$ an independent iid sample from a $N(\beta,3\sigma^2)$ population.

(a) Is this a general linear model? Explain carefully.

(b) If your answer to (a) was "yes," then find the least-squares estimator of $\beta$, and write a t-test for $H_0: \beta = 0$.  Your test must be described explicitly in terms of the $Y_{i,j}$'s.

## Solution
(a) Yes, it is a general linear model $Y=X \beta + \varepsilon$ if we make one small adjustment to achieve $\varepsilon_{i,j} \sim N(0,\sigma^2)$, since as originally stated, $\varepsilon_{2,j} \sim N(0,3\sigma^2)$. Let the original $Y_{2,j} \;(\sim N(\beta,3\sigma^2))$ be denoted $\widetilde{Y}_{2,j}\;(\sim N(\beta,3\sigma^2))$ and define our new $Y_{2,j} = \frac{\widetilde{Y}_{2,j}}{\sqrt3}\;(\sim N(\beta,\sigma^2))$
so we have
$$Y'=[Y_{1,1},...,Y_{1,n},Y_{2,1},...,Y_{2,n}],\;\;\;X' = \left[1, ...,1,\frac{1}{\sqrt3},...,\frac{1}{\sqrt3}\right]_{1\times2n},\;\;\beta=\mu,\;\;\; \varepsilon'=[\varepsilon_{1,1},...,\varepsilon_{1,n},\varepsilon_{2,1},...,\varepsilon_{2,n}]$$
where $Y$, $X$, and $\varepsilon$ are $2n \times 1$ and $\beta$ is $1 \times 1$, so $p=1$.

(b) Then the least-squares estimate of $\beta$ is
$$\hat{\beta} = (X'X)^{-1}X'Y.$$
We have
$$X'X = n + \frac{1}{3}n$$
$$X'Y = \sum_{j=1}^{n}Y_{1,j} + \frac{1}{\sqrt3}\sum_{j=1}^{n}Y_{2,j}$$
so we have 
$$\hat{\beta} = \frac{3}{4n} \left(\sum_{j=1}^{n}Y_{1,j} + \frac{1}{\sqrt3}\sum_{j=1}^{n}Y_{2,j}\right).$$
We know a $100(1-\alpha)\%$ confidence interval for $\beta$ is given by 
$$\hat{\beta} \pm St^{\alpha/2}_{2n-1} \sqrt{X'X}$$
and $S^2:= \frac{1}{2n-1} RSS$ is computed
$$S^2= \frac{1}{2n-1} \left[\sum_{j=1}^{n}(Y_{1,j}-\hat{\beta})^2 + \sum_{j=1}^{n}\left(Y_{2,j}-\frac{\hat{\beta}}{\sqrt3}\right)^2\right].$$
